{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"UV Data Science Project Mono-Repository Template","text":"<p>Welcome to the documentation for UV Data Science Project Mono-Repository Template. This project demonstrates how to set up a data science environment using Docker, UV, FastAPI, along with other tools for developing python projects.</p> <p>Image by David T. [Source: Astral]</p> <p> </p> <p>Tutorial for a MonoRepo Project with UV for Python / Data Science</p> <p>What I mean by a monorepo:</p> <ul> <li>2+ packages with interdependencies using the Cargo concept by utilizing Workspaces with UV.</li> <li>The ability to lock dependencies across packages (where not needed, split into multiple workspaces). More sophisticated multi-version handling would be great but out of scope.</li> <li>The main package is defined in the <code>src</code> folder of the <code>root</code> project, while other packages are defined under the <code>packages</code> folder.</li> <li>Multiple packages with single lockfile.</li> <li>Dependencies between workspace members are editable.</li> </ul> <p>This UV Setup supports the given scope.</p> <p>General Tutorial Project for 1) Developing Data Science Projects in a Dev Container, and 2) Machine Learning Applications in Production</p> <p>This guide provides instructions on how to develop and productionize machine learning applications in a robust and efficient way. It is demonstrated how to achieve this using a modern setup of tools, like UV, Docker, Ruff, FastAPI and more (see Overview Tools Section). The focus of this project is to give an introduction to using those tools and not on how to properly set up a machine learning application (for production). Therefore only a simple machine learning pipeline based on PyTorch/Lightning and FastAPI is used.</p> <p>See the related Project Documentation for additional information.</p>"},{"location":"#cargo-concept-utilized-by-uv-workspaces","title":"Cargo Concept utilized by UV Workspaces","text":"<p>See to UV documentation for Using Workspaces and Getting started with Workspaces.</p>"},{"location":"#monorepo-concept","title":"MonoRepo Concept","text":"<p>Inspired by the Cargo concept of the same name, a workspace is \"a collection of one or more packages, called workspace members, that are managed together.\"</p> <p>Workspaces organize large codebases by splitting them into multiple packages with common dependencies. Think: a FastAPI-based web application, alongside a series of libraries that are versioned and maintained as separate Python packages, all in the same Git repository.</p> <p>In a workspace, each package defines its own <code>pyproject.toml</code>, but the workspace shares a single lockfile, ensuring that the workspace operates with a consistent set of dependencies.</p> <p>As such, <code>uv lock</code> operates on the entire workspace at once, while <code>uv run</code> and <code>uv sync</code> operate on the workspace root by default, though both accept a <code>--package</code> argument, allowing you to run a command in a particular workspace member from any workspace directory.</p> <pre><code>.\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 uv.lock                           # single lockfile for multiple packages.\n\u251c\u2500\u2500 ...                               # overall project setup files like LICENSE, Dockerfile, pytest.ini, ruff.toml, ... .\n\u251c\u2500\u2500 src/uv_datascience_project_monorepo_template    # is a packaged application in this case, but can also be a lib.\n|   \u251c\u2500\u2500 __init__.py\n|   \u251c\u2500\u2500 main.py\n|   \u2514\u2500\u2500 train_autoencoder.py\n\u251c\u2500\u2500 tests\n|   \u2514\u2500\u2500 ...\n|\n\u2514\u2500\u2500 packages\n \u00a0\u00a0 \u251c\u2500\u2500 lit-auto-encoder              # is a lib.\n \u00a0\u00a0 \u2502   \u251c\u2500\u2500 pyproject.toml\n \u00a0\u00a0 \u2502   \u251c\u2500\u2500 ...                       # packages specific files like \"tests/\".\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 src/lit_auto_encoder\n \u00a0\u00a0 \u2502       \u251c\u2500\u2500 __init__.py\n \u00a0\u00a0 \u2502       \u251c\u2500\u2500 lit_auto_encoder.py\n \u00a0\u00a0 \u2502       \u2514\u2500\u2500 train_autoencoder.py\n \u00a0\u00a0 \u2514\u2500\u2500 mylib\n \u00a0\u00a0     \u251c\u2500\u2500 pyproject.toml\n        \u251c\u2500\u2500 ...\n \u00a0\u00a0     \u2514\u2500\u2500 mylib\n \u00a0\u00a0         \u2514\u2500\u2500 __init__.py\n</code></pre> <pre><code>[project]\nname = \"uv-datascience-project-monorepo-template\"\nversion = \"0.1.0\"\nrequires-python = \"&gt;=3.12.0, &lt;3.13.0\"\ndependencies = [\n    \"lit-auto-encoder\",   # monorepo\n    \"fastapi[standard]&gt;=0.115.6\",\n    \"pydantic&gt;=2.10.4\",\n    \"uvicorn&gt;=0.34.0\"\n]\n\n[tool.uv.sources]   # monorepo\nlit-auto-encoder = { workspace = true }\n\n[tool.uv.workspace]   # monorepo\nmembers = [\"packages/*\"]\nexclude = [\"packages/mylib\"]\n\n# Defines the entry point of the packaged application\n[project.scripts]\nhello = \"uv_datascience_project_monorepo_template:main\"\n</code></pre> <p>[!NOTE] The given use case only has a single direct dependency (\"uv-datascience-project-monorepo-template\" depends on \"lit-auto-encoder\"). A workspace setup is not needed for this case, but still shows how to define such a setup. Workspaces are intended to facilitate the development of multiple interconnected packages within a single repository. When (not) to use workspaces</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>By default, <code>uv run</code> and <code>uv sync</code> operates on the workspace root. For example, in the above example, <code>uv run</code> and <code>uv run --package uv-datascience-project-monorepo-template</code> would be equivalent, while <code>uv run --package lit-auto-encoder</code> would run the command in the <code>lit-auto-encoder</code> package.</p> <p>Since a command definition <code>[project.scripts]</code> is included, the command can be run from a console (ensure your <code>__init__.py</code> is setup correctly):</p> <pre><code>uv run hello\n</code></pre> <p>[!NOTE] To properly build (<code>.tar.gz</code> and <code>.whl</code>) the application with the related libraries defined under packages, use:</p> <pre><code>uv build\nuv build --all-packages\n</code></pre> <p>Install packages in your root folder with:</p> <ul> <li> <p>Install packages in root (\"uv-datascience-project-monorepo-template\"):</p> <pre><code>uv add --group dev pytest\n</code></pre> </li> <li> <p>Install packages for a workspace package (e.g. \"lit-auto-encoder\"):</p> <pre><code>uv add --package lit-auto-encoder --group dev pytest\n</code></pre> </li> </ul> <p>Sync packages in your root folder with:</p> <pre><code>uv sync --all-packages\n</code></pre>"},{"location":"#overview-tools","title":"Overview Tools","text":"<p>The project includes the following components, for more details see Documentation - Guides:</p> Tool Description UV A fast and efficient package manager for Python, written in Rust. It replaces tools like pip and virtualenv. Ruff An extremely fast Python linter, formatter, and code assistant, written in Rust. PyRight A static type checker for Python, helping to catch type-related errors early in the development process. PyTest A powerful and flexible testing framework for Python, simplifying writing and running tests. Coverage A tool for measuring code coverage of Python programs, helping to ensure that all parts of the code are tested. Pre-Commit A framework for managing and maintaining multi-language pre-commit hooks to ensure code quality. CI-GitHub Continuous Integration setup using GitHub Actions to automate testing, linting, and deployment. MkDocs A static site generator geared towards building project documentation, written in Markdown. VSCode-DevContainer A development environment setup using Docker and VS Code, providing a consistent and isolated workspace. Docker-Production Docker setup for creating a lean, efficient, and secure production environment for applications."},{"location":"#using-uv-to-manage-the-project","title":"Using uv to Manage the Project","text":"<p><code>UV</code> is a tool that simplifies the management of Python projects and virtual environments. It handles dependency installation, virtual environment creation, and other project configurations. In this project, <code>UV</code> is used to manage dependencies and the virtual environment inside the Docker container, ensuring a consistent and reproducible setup.</p> <p>See Guides - UV for a comprehensive guide.</p>"},{"location":"#pyproject-toml","title":"pyproject toml","text":"<p>The <code>pyproject.toml</code> file includes the following sections:</p> <ol> <li>Project metadata (name, version, description, etc.).</li> <li>Dependencies required for the project.</li> <li>Dependency groups for development and documentation.</li> <li>Configuration for tools and packaging.</li> </ol>"},{"location":"#custom-code-in-src-folder","title":"Custom Code in src Folder","text":"<p>See Source Code API Reference for a comprehensive documentation.</p> <p>The <code>src</code> folder and the <code>packages/lit-auto-encoder/src</code> folder contains the custom code for the machine learning project. The main components include:</p>"},{"location":"#lit_auto_encoder","title":"lit_auto_encoder","text":"<p>This file defines the <code>LitAutoEncoder</code> class, which is a LightningModule an autoencoder using PyTorch Lightning. The <code>LitAutoEncoder</code> class includes:</p> <ol> <li>An <code>__init__</code> method to initialize the encoder and decoder.</li> <li>A <code>training_step</code> method to define the training loop.</li> <li>A <code>configure_optimizers</code> method to set up the optimizer.</li> </ol>"},{"location":"#train_autoencoder","title":"train_autoencoder","text":"<p>This file defines the training function <code>train_litautoencoder</code> to initialize and train the model on the MNIST dataset using PyTorch Lightning.</p>"},{"location":"#fastapi-application","title":"FastAPI Application","text":"<p>The FastAPI application is defined in the <code>app_fastapi_autoencoder.py</code> file. It includes the following endpoints:</p> <ol> <li><code>GET /</code>: Root endpoint that provides a welcome message and instructions.</li> <li><code>POST /train</code>: Endpoint to train the autoencoder model.</li> <li><code>POST /embed</code>: Endpoint to embed fake images using the trained autoencoder.</li> </ol>"},{"location":"#app_fastapi_autoencoder","title":"app_fastapi_autoencoder","text":"<p>See Source Code API Reference for a comprehensive documentation.</p> <p>This file defines the FastAPI application and the endpoints. It includes:</p> <ol> <li>Importing necessary libraries and modules.</li> <li>Defining global variables for the encoder, decoder, and model training status.</li> <li>A <code>NumberFakeImages</code> class for input validation.</li> <li>A <code>train_litautoencoder</code> function to initialize and train the autoencoder.</li> <li>A <code>read_root</code> function to handle the root endpoint.</li> <li>A <code>train_model</code> function to handle the model training endpoint.</li> <li>An <code>embed</code> function to handle the embedding endpoint.</li> <li>The application entry point to run the FastAPI application.</li> </ol>"},{"location":"#main","title":"main","text":"<p>This file defines the uvicorn server to run the FastAPI AutoEncoder application and the endpoints. It includes:</p> <ol> <li>Importing necessary libraries and modules, including the source code of the project.</li> <li>The application entry point to run the FastAPI application.</li> </ol> <pre><code># filepath: src/uv_datascience_project_monorepo_template/main.py\n\ndef main() -&gt; None:\n    \"\"\"Run the FastAPI application.\"\"\"\n    uvicorn.run(app=app, host=\"0.0.0.0\", port=8000)\n\n# Application entry point\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"#production-setup-for-the-machine-learning-fastapi-app-hosted-in-the-docker-container","title":"Production Setup for the Machine Learning FastAPI App hosted in the Docker container","text":"<p>See Docker Production Setup for a comprehensive guide.</p>"},{"location":"#dockerfile","title":"Dockerfile","text":"<p>The <code>Dockerfile</code> is used to build the Docker image for the project. It includes the following steps:</p> <ol> <li>Define build-time arguments for the base container images and workspace name.</li> <li>Use a Python image with <code>uv</code> pre-installed.</li> <li>Set the working directory.</li> <li>Enable bytecode compilation for faster startup.</li> <li>Copy and install dependencies without installing the project.</li> <li>Copy the application source code and install it.</li> <li>Add executables and source to environment paths.</li> <li>Set the default command to run the FastAPI application.</li> </ol>"},{"location":"#multi-stage-dockerfile","title":"Multi Stage Dockerfile","text":"<p>To build the multistage image for a container optimized final image without uv use the <code>multistage.Dockerfile</code>.</p>"},{"location":"#docker-compose","title":"Docker Compose","text":"<p>The <code>docker-compose.yml</code> file is used to define and run multi-container Docker applications. It includes the following configurations:</p> <ol> <li>Build the image from the <code>Dockerfile</code>.</li> <li>Define the image name.</li> <li>Host the FastAPI application on port 8000.</li> <li>Mount the current directory to the app directory in the container.</li> <li>Set environment variables.</li> <li>Define the default command to start the FastAPI application.</li> </ol>"},{"location":"#build-the-docker-image-and-run-a-container","title":"Build the docker image and run a container","text":"<p>Build and run a specific or all services when multiple services (\"app\" and \"app-optimized-docker\") are defined in <code>docker-compose.yml</code>. Note that in the give example both services us the same port and only one service at a time should be used.</p> <pre><code>docker-compose up --build\n</code></pre> <p>or to build a single service only \"app\" respectively \"app-optimized-docker\".</p> <pre><code>docker-compose up --build app\n</code></pre> <pre><code>docker-compose up --build app-optimized-docker\n</code></pre>"},{"location":"#test-the-endpoint-with-curl","title":"Test the endpoint with curl","text":"<ul> <li> <p>Welcome root endpoint</p> <pre><code>curl -X GET http://0.0.0.0:8000/\n</code></pre> </li> <li> <p>Get docs of the request options of the FastAPI app:</p> <pre><code>curl -X GET http://0.0.0.0:8000/docs\n</code></pre> </li> <li> <p>Test the endpoint with curl by training the model first, followed by requesting predictions for n fake images</p> <pre><code>curl -X POST http://0.0.0.0:8000/train \\\ncurl -X POST http://0.0.0.0:8000/embed -H \"Content-Type: application/json\" -d '{\"n_fake_images\": 4}'\n</code></pre> </li> </ul>"},{"location":"#development-in-dev-container","title":"Development in Dev Container","text":"<p>See VSCode Dev-Container (Docker) Setup for Data Science Projects using UV for a comprehensive guide.</p> <ul> <li>Run the server: <code>uv run /workspace/main.py</code></li> <li> <p>Test the standard endpoints with curl:</p> <ul> <li> <p>Get docs of the request options of the FastAPI app</p> <pre><code>curl -X GET http://localhost:8000/docs\n</code></pre> </li> <li> <p>Welcome root request of the FastAPI app, providing an app description</p> <pre><code>curl -X GET http://localhost:8000/\n</code></pre> </li> <li> <p>Test the machine learning endpoints with curl:</p> <pre><code>curl -X POST http://localhost:8000/train \\\ncurl -X POST http://localhost:8000/embed -H \"Content-Type: application/json\" -d '{\"n_fake_images\": 1}'\n</code></pre> </li> </ul> </li> </ul>"},{"location":"#conclusion","title":"Conclusion","text":"<p>This repository provides a comprehensive overview of setting up and running the machine learning FastAPI project using Docker and <code>uv</code>. Follow the instructions to build and run the application in both development and production environments. The project demonstrates how to develop and productionize machine learning applications using modern tools and best practices.</p> <p>Additionally, ensure to review the provided guides and documentation for detailed instructions on various setups and configurations necessary for optimal project performance.</p>"},{"location":"api/","title":"Source Code API Reference Overview","text":""},{"location":"api/#autoencoder","title":"Autoencoder","text":""},{"location":"api/#training","title":"Training","text":""},{"location":"api/#fastapi","title":"FastAPI","text":""},{"location":"api/autoencoder/","title":"Autoencoder","text":""},{"location":"api/autoencoder/#lit_auto_encoder.auto_encoder.LitAutoEncoder","title":"<code>LitAutoEncoder</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A simple autoencoder model.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Sequential</code> <p>The encoder component, responsible for encoding input data.</p> required <code>decoder</code> <code>Sequential</code> <p>The decoder component, responsible for decoding encoded data.</p> required Source code in <code>packages/lit-auto-encoder/src/lit_auto_encoder/auto_encoder.py</code> <pre><code>class LitAutoEncoder(L.LightningModule):\n    \"\"\"A simple autoencoder model.\n\n    Args:\n        encoder: The encoder component, responsible for encoding input data.\n        decoder: The decoder component, responsible for decoding encoded data.\n    \"\"\"\n\n    def __init__(self, encoder: nn.Sequential, decoder: nn.Sequential) -&gt; None:\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def training_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -&gt; Tensor:\n        \"\"\"Performs a single training step for the model.\n\n        Args:\n            batch (Tuple[Tensor, Tensor]): A tuple containing the input data (x) and\n                the corresponding labels (y).\n            batch_idx (int): The index of the current batch.\n\n        Returns:\n            Tensor: The computed loss for the current training step.\n        \"\"\"\n\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = nn.functional.mse_loss(x_hat, x)\n        # Logging to TensorBoard (if installed) by default\n        # self.log(\"train_loss\", loss)\n        return loss\n\n    def configure_optimizers(self) -&gt; optim.Adam:\n        \"\"\"Configure the Adam optimizer.\"\"\"\n        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n</code></pre>"},{"location":"api/autoencoder/#lit_auto_encoder.auto_encoder.LitAutoEncoder.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure the Adam optimizer.</p> Source code in <code>packages/lit-auto-encoder/src/lit_auto_encoder/auto_encoder.py</code> <pre><code>def configure_optimizers(self) -&gt; optim.Adam:\n    \"\"\"Configure the Adam optimizer.\"\"\"\n    optimizer = optim.Adam(self.parameters(), lr=1e-3)\n    return optimizer\n</code></pre>"},{"location":"api/autoencoder/#lit_auto_encoder.auto_encoder.LitAutoEncoder.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Performs a single training step for the model.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>A tuple containing the input data (x) and the corresponding labels (y).</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The computed loss for the current training step.</p> Source code in <code>packages/lit-auto-encoder/src/lit_auto_encoder/auto_encoder.py</code> <pre><code>def training_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -&gt; Tensor:\n    \"\"\"Performs a single training step for the model.\n\n    Args:\n        batch (Tuple[Tensor, Tensor]): A tuple containing the input data (x) and\n            the corresponding labels (y).\n        batch_idx (int): The index of the current batch.\n\n    Returns:\n        Tensor: The computed loss for the current training step.\n    \"\"\"\n\n    x, y = batch\n    x = x.view(x.size(0), -1)\n    z = self.encoder(x)\n    x_hat = self.decoder(z)\n    loss = nn.functional.mse_loss(x_hat, x)\n    # Logging to TensorBoard (if installed) by default\n    # self.log(\"train_loss\", loss)\n    return loss\n</code></pre>"},{"location":"api/fastapi_app/","title":"FastAPI App","text":""},{"location":"api/fastapi_app/#uv_datascience_project_monorepo_template.app_fastapi_autoencoder.embed","title":"<code>embed(input_data)</code>","text":"<p>Embed fake images using the trained autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>NumberFakeImages</code> <p>Input data containing the number of fake images to embed.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the embeddings of the fake images.</p> Source code in <code>src/uv_datascience_project_monorepo_template/app_fastapi_autoencoder.py</code> <pre><code>@app.post(\"/embed\")\ndef embed(input_data: NumberFakeImages) -&gt; dict[str, Any]:\n    \"\"\"Embed fake images using the trained autoencoder.\n\n    Args:\n        input_data (NumberFakeImages): Input data containing the number of fake images to embed.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the embeddings of the fake images.\n    \"\"\"\n    global encoder, decoder\n\n    if encoder is None or decoder is None:\n        raise HTTPException(\n            status_code=500, detail=\"Model not initialized. Train the model first.\"\n        )\n\n    n_fake_images = input_data.n_fake_images\n    checkpoint_path = \"./lightning_logs/LitAutoEncoder/version_0/checkpoints/epoch=0-step=100.ckpt\"\n\n    if not os.path.exists(checkpoint_path):\n        raise HTTPException(\n            status_code=500, detail=\"Checkpoint file not found. Train the model first.\"\n        )\n\n    # Load the trained autoencoder from the checkpoint\n    autoencoder = LitAutoEncoder.load_from_checkpoint(\n        checkpoint_path, encoder=encoder, decoder=decoder\n    )\n    encoder_model = autoencoder.encoder\n    encoder_model.eval()\n\n    # Generate fake image embeddings based on user input\n    fake_image_batch = rand(n_fake_images, 28 * 28, device=autoencoder.device)\n    embeddings = encoder_model(fake_image_batch)\n    # print(\"\u26a1\" * 20, \"\\nPredictions (image embeddings):\\n\", embeddings, \"\\n\", \"\u26a1\" * 20)\n\n    return {\"embeddings\": embeddings.tolist()}\n</code></pre>"},{"location":"api/fastapi_app/#uv_datascience_project_monorepo_template.app_fastapi_autoencoder.read_root","title":"<code>read_root()</code>","text":"<p>Root endpoint that provides information about the API.</p> Source code in <code>src/uv_datascience_project_monorepo_template/app_fastapi_autoencoder.py</code> <pre><code>@app.get(\"/\")\ndef read_root() -&gt; Response:\n    \"\"\"Root endpoint that provides information about the API.\"\"\"\n\n    message = \"\"\"\n    \u26a1\u26a1\u26a1 Welcome to the LitAutoEncoder API! \u26a1\u26a1\u26a1\n    - To train the model, send a POST request to '/train' without providing any additional input.\n    - To get encodings for random fake images, POST to '/embed' with JSON input: \n      {'n_fake_images': [1-10]} in the request body.\n    \"\"\"\n    return Response(content=message, media_type=\"text/plain\")\n</code></pre>"},{"location":"api/fastapi_app/#uv_datascience_project_monorepo_template.app_fastapi_autoencoder.train_model","title":"<code>train_model()</code>","text":"<p>Train the autoencoder model.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: A message indicating the training status.</p> Source code in <code>src/uv_datascience_project_monorepo_template/app_fastapi_autoencoder.py</code> <pre><code>@app.post(\"/train\")\ndef train_model() -&gt; dict[str, str]:\n    \"\"\"Train the autoencoder model.\n\n    Returns:\n        dict[str, str]: A message indicating the training status.\n    \"\"\"\n    global encoder, decoder, is_model_trained\n\n    if is_model_trained:\n        return {\"message\": \"Model is already trained.\"}\n\n    encoder, decoder, is_model_trained = train_litautoencoder()\n    return {\"message\": \"Model training completed successfully.\"}\n</code></pre>"},{"location":"api/training/","title":"Training","text":""},{"location":"api/training/#lit_auto_encoder.train_autoencoder.train_litautoencoder","title":"<code>train_litautoencoder()</code>","text":"<p>Trains a LitAutoEncoder model on the MNIST dataset and returns the trained encoder, decoder, and a flag indicating training completion.</p> <p>Returns:</p> Type Description <code>tuple[Sequential, Sequential, Literal[True]]</code> <p>tuple[Sequential, Sequential, Literal[True]]: A tuple containing the trained encoder, decoder, and a boolean flag indicating that the model has been successfully trained.</p> Source code in <code>packages/lit-auto-encoder/src/lit_auto_encoder/train_autoencoder.py</code> <pre><code>def train_litautoencoder() -&gt; tuple[Sequential, Sequential, Literal[True]]:\n    \"\"\"Trains a LitAutoEncoder model on the MNIST dataset and returns\n    the trained encoder, decoder, and a flag indicating training completion.\n\n    Returns:\n        tuple[Sequential, Sequential, Literal[True]]: A tuple containing the trained encoder,\n            decoder, and a boolean flag indicating that the model has been successfully trained.\n    \"\"\"  # noqa: D205\n\n    # Define the encoder and decoder architecture\n    encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n    decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\n    # Initialize the LitAutoEncoder\n    autoencoder = LitAutoEncoder(encoder, decoder)\n\n    # Load the MNIST dataset\n    dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\n    train_loader = utils.data.DataLoader(dataset)\n\n    # Initialize the TensorBoard logger\n    logger = CSVLogger(\"lightning_logs\", name=\"LitAutoEncoder\")\n\n    # Train the autoencoder\n    trainer = L.Trainer(limit_train_batches=100, max_epochs=1, logger=logger)\n    trainer.fit(model=autoencoder, train_dataloaders=train_loader)\n\n    is_model_trained = True  # Mark model as trained\n\n    return encoder, decoder, is_model_trained\n</code></pre>"},{"location":"guides/","title":"Overview Tools","text":"<p>The project includes the following components:</p> Tool Description UV A fast and efficient package manager for Python, written in Rust. It replaces tools like pip and virtualenv. Ruff An extremely fast Python linter, formatter, and code assistant, written in Rust. PyRight A static type checker for Python, helping to catch type-related errors early in the development process. PyTest A powerful and flexible testing framework for Python, simplifying writing and running tests. Coverage A tool for measuring code coverage of Python programs, helping to ensure that all parts of the code are tested. Pre-Commit A framework for managing and maintaining multi-language pre-commit hooks to ensure code quality. CI-GitHub Continuous Integration setup using GitHub Actions to automate testing, linting, and deployment. MkDocs A static site generator geared towards building project documentation, written in Markdown. VSCode-DevContainer A development environment setup using Docker and VS Code, providing a consistent and isolated workspace. Docker-Production Docker setup for creating a lean, efficient, and secure production environment for applications."},{"location":"guides/ci_github/","title":"Continuous Integration (CI)","text":"<p>Continuous Integration is a development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run. This helps to detect integration errors early and ensures that the codebase remains in a working state.</p> <ul> <li>Continuous Integration (CI)</li> <li>Integration of Workflows and Actions</li> <li>CI Workflows in this Project</li> <li>Custom GitHub Actions<ul> <li><code>setup-python-with-uv</code></li> <li><code>setup-git-config</code></li> </ul> </li> <li>Getting Started: Set Up GitHub Actions and Workflows</li> </ul>"},{"location":"guides/ci_github/#integration-of-workflows-and-actions","title":"Integration of Workflows and Actions","text":"<p>The CI process is orchestrated through GitHub Actions workflows (defined in the <code>.github/workflows</code> directory). Each workflow defines a series of jobs, which in turn consist of steps. These steps can either be individual commands or calls to reusable actions.</p> <p>For example, the <code>test.yml</code> workflow uses the <code>setup-python-with-uv</code> action to set up the Python environment before running tests. Similarly, the <code>ruff.yml</code> workflow uses <code>setup-python-with-uv</code> before running the Ruff linter and formatter.</p> <p>This modular approach allows for a clear and maintainable CI configuration, where common setup tasks are encapsulated in reusable actions, and workflows define the overall CI process.</p>"},{"location":"guides/ci_github/#ci-workflows-in-this-project","title":"CI Workflows in this Project","text":"<p>This project uses GitHub Actions for CI Workflows. Upon each push or pull request to the <code>main</code> branch, the following steps, defined in <code>.github/workflows</code> directory, are executed:</p> <ol> <li> <p>Linting:</p> <ul> <li>Ruff: The code is checked for stylistic errors, potential bugs, and adherence to coding standards using Ruff. This includes both linting and formatting checks.</li> <li>Hadolint: The <code>Dockerfile</code> and <code>.devcontainer/Dockerfile.debug</code> are checked for errors and best practices using Hadolint.</li> </ul> </li> <li> <p>Type Checking:</p> <ul> <li>Pyright: Static type checking is performed using Pyright to catch type-related errors.</li> </ul> </li> <li> <p>Testing:</p> <ul> <li>Pytest: Unit tests are executed using Pytest to verify the correctness of individual components.</li> <li>Pytest-Coverage: Coverage reports are generated and posted as comments on pull requests using Pytest-cov.</li> </ul> </li> <li> <p>Building:</p> <ul> <li>Docker: The project's Docker image is built to ensure that all dependencies are correctly resolved and that the application can be containerized successfully. This includes building both the main <code>Dockerfile</code> and the development container <code>Dockerfile.debug</code>.</li> </ul> </li> <li> <p>Documentation Deployment: The project documentation is automatically built and deployed to GitHub Pages using mkdocs.</p> </li> <li> <p>CI Workflow Status: Provides a summary of the CI workflow status, passed and failed jobs.</p> </li> </ol> <p>You can check the status of the latest CI run on the GitHub repository under the \"Actions\" tab.</p>"},{"location":"guides/ci_github/#custom-github-actions","title":"Custom GitHub Actions","text":"<p>This project utilizes custom GitHub Actions to encapsulate reusable steps within the CI Workflows. These actions are defined in the <code>.github/actions</code> directory.</p>"},{"location":"guides/ci_github/#setup-python-with-uv","title":"<code>setup-python-with-uv</code>","text":"<p>This action sets up a Python environment using the <code>uv</code> package manager. It handles:</p> <ul> <li>Installing <code>uv</code>.</li> <li>Pinning the specified Python version.</li> <li>Caching <code>uv</code> files to speed up subsequent runs.</li> <li>Installing project dependencies using <code>uv sync</code>.</li> </ul>"},{"location":"guides/ci_github/#setup-git-config","title":"<code>setup-git-config</code>","text":"<p>This action configures Git user name and email. It is used to:</p> <ul> <li>Set the Git user name to <code>github-actions[bot]</code>.</li> <li>Set the Git user email to <code>41898282+github-actions[bot]@users.noreply.github.com</code>.</li> </ul>"},{"location":"guides/ci_github/#getting-started-set-up-github-actions-and-workflows","title":"Getting Started: Set Up GitHub Actions and Workflows","text":"<p>To set up GitHub Actions and Workflows in your project, follow these steps:</p> <ul> <li>Create a <code>.github/workflows</code> directory in the root of your repository.</li> <li> <p>Define your workflows in YAML files within this directory.       Example: <code>.github/workflows/pyright.yml</code></p> <pre><code>name: Pyright\n\non:\npush:\nbranches: [main]\npull_request:\nbranches: [main]\n\njobs:\ntype-check:\nruns-on: ubuntu-latest\n\nstrategy:\n      matrix:\n      python-version: [\"3.9\", \"3.10\", \"3.11\", \"3.12\", \"3.13\"]\n\nsteps:\n      - uses: jakebailey/pyright-action@v2\n            with:\n            python-version: ${{ matrix.python-version }}\n</code></pre> </li> <li> <p>Define jobs and steps within each workflow file.</p> </li> <li> <p>Use reusable actions to encapsulate common steps.       You can use existing actions from the GitHub Marketplace or create custom actions.       Example: <code>.github/actions/setup-python-with-uv/action.yml</code></p> <pre><code>name: Install Python with uv\ndescription: |\nThis GitHub Action installs Python using uv.\nIt pins the specified Python version, caches uv files, and installs dependencies.\n\ninputs:\npython-version:\ndescription: Python version\nrequired: true\n\nruns:\nusing: composite\nsteps:\n- name: Install uv\n      uses: astral-sh/setup-uv@v4\n      with:\n      enable-cache: true\n      python-version: ${{ inputs.python-version }}\n\n- name: Install Dependencies\n      run: uv sync --all-groups\n      shell: bash\n</code></pre> </li> <li> <p>Commit and push your changes to trigger the workflows.       <pre><code>git add .\ngit commit -m \"Set up GitHub Actions for Pyright\"\ngit push origin main\n</code></pre></p> </li> <li>Monitor the Actions tab in your GitHub repository to check the status of your workflows.       Go to the \"Actions\" tab in your GitHub repository to check the status of your CI workflows. You should see the workflows running based on the configuration in your workflow files.</li> </ul> <p>By following these steps, you can set up GitHub Actions for Continuous Integration in your project, ensuring that your codebase is automatically tested and built upon each push or pull request.</p>"},{"location":"guides/docker_prod/","title":"Docker Production Setup","text":"<p>This document provides an in-depth guide to the Docker production setup used in this project. It covers the rationale behind the chosen approach, the structure of the <code>Dockerfile</code> respectively <code>multistage.Dockerfile</code> and <code>docker-compose.yml</code> files, and best practices for deploying the application in a production environment.</p> <ul> <li>Docker Production Setup</li> <li>Overview<ul> <li>Key Components</li> </ul> </li> <li><code>Dockerfile</code> Explained<ul> <li>1. Base Image</li> <li>2. Working Directory</li> <li>3. Dependency Installation</li> <li>4. Copying Application Code</li> <li>5. Installing Application</li> <li>6. Environment Variables</li> <li>7. Entrypoint and Command</li> </ul> </li> <li><code>multistage.Dockerfile</code> Explained<ul> <li>1. Builder Stage Image</li> <li>2. Final Image</li> </ul> </li> <li><code>docker-compose.yml</code> Explained<ul> <li>Build Arguments</li> <li>1. Standard Services</li> <li>2. Optimized Docker Service</li> </ul> </li> <li>Build and Run and Test the Docker Application</li> <li>Best Practices</li> <li>Getting Started</li> <li>Conclusion</li> </ul>"},{"location":"guides/docker_prod/#overview","title":"Overview","text":"<p>The Docker setup is designed to create a lean, efficient, and secure production environment for the application. It leverages multi-stage builds to minimize the final image size and uses <code>uv</code> for fast and reliable dependency management.</p>"},{"location":"guides/docker_prod/#key-components","title":"Key Components","text":"<ol> <li> <p><code>Dockerfile</code>: Defines the steps to build the Docker image. It includes:</p> </li> <li> <p>Base image selection.</p> </li> <li>Dependency installation using <code>uv</code>.</li> <li>Copying application code.</li> <li>Setting environment variables.</li> <li> <p>Defining the entry point and command.</p> </li> <li> <p><code>multistage.Dockerfile</code>: Defines an optimized image to reduce image size. Custom python code should be a packaged application.</p> </li> <li> <p><code>docker-compose.yml</code>: Defines the services, networks, and volumes for the application. It includes:</p> </li> <li> <p>Service definitions for the application.</p> </li> <li>Port mappings.</li> <li>Environment variables.</li> <li>Build configurations.</li> </ol>"},{"location":"guides/docker_prod/#dockerfile-explained","title":"<code>Dockerfile</code> Explained","text":"<p>The standard <code>Dockerfile</code> is designed to create a production-ready image that includes all necessary dependencies and the application code. It leverages the uv package manager for efficient dependency management and environment setup. The <code>Dockerfile</code> is structured to optimize for layer caching and minimize the final image size. Here's a breakdown of the key sections:</p>"},{"location":"guides/docker_prod/#1-base-image","title":"1. Base Image","text":"<p>We use a <code>uv</code> pre-installed Python image with a Debian base. The <code>UV_VER</code> argument allows us to specify the Python version at build time.</p> <pre><code>FROM ghcr.io/astral-sh/uv:$UV_VER AS uv\n</code></pre>"},{"location":"guides/docker_prod/#2-working-directory","title":"2. Working Directory","text":"<p>Sets the working directory inside the container.</p> <pre><code>WORKDIR /${WORKSPACE_NAME}\n</code></pre>"},{"location":"guides/docker_prod/#3-dependency-installation","title":"3. Dependency Installation","text":"<pre><code>RUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --frozen --no-install-workspace --no-dev\n</code></pre> <p>Installs the project dependencies using <code>uv sync</code>:</p> <ul> <li><code>--frozen</code> ensures that the exact versions specified in <code>uv.lock</code> are used.</li> <li><code>--no-install-project</code> prevents the project itself from being installed at this stage.</li> <li><code>--no-dev</code> excludes development dependencies.</li> <li><code>--mount=type=cache,target=/root/.cache/uv</code> caches the uv environment.</li> <li><code>--mount=type=bind,source=uv.lock,target=uv.lock</code> binds the uv.lock file to the container.</li> <li><code>--mount=type=bind,source=pyproject.toml,target=pyproject.toml</code> binds the pyproject.toml file to the container.</li> </ul>"},{"location":"guides/docker_prod/#4-copying-application-code","title":"4. Copying Application Code","text":"<p>Copies the application code into the container.</p> <pre><code>COPY . /${WORKSPACE_NAME}\n</code></pre>"},{"location":"guides/docker_prod/#5-installing-application","title":"5. Installing Application","text":"<p>Installs the project itself. <code>--mount=type=cache,target=/root/.cache/uv</code> caches the uv environment.</p> <pre><code>RUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --all-packages --frozen --no-dev\n</code></pre>"},{"location":"guides/docker_prod/#6-environment-variables","title":"6. Environment Variables","text":"<p>Adds the virtual environment's <code>bin</code> directory to the <code>PATH</code> environment variable, ensuring that the correct executables are used.</p> <pre><code>ENV PATH=\"/${WORKSPACE_NAME}/.venv/bin:$PATH\"\n</code></pre>"},{"location":"guides/docker_prod/#7-entrypoint-and-command","title":"7. Entrypoint and Command","text":"<p>Sets the default command to run when the container starts, which in this case is starting the FastAPI application using <code>uvicorn</code>.</p> <pre><code>ENTRYPOINT []\nCMD [\"uv\", \"run\", \"hello\"]\n</code></pre>"},{"location":"guides/docker_prod/#multistagedockerfile-explained","title":"<code>multistage.Dockerfile</code> Explained","text":"<p>The <code>multistage.Dockerfile</code> is designed to create an optimized production image by separating the build and runtime environments. This approach reduces the final image size and ensures that only the necessary components are included in the runtime image. Here's a breakdown of the key sections:</p>"},{"location":"guides/docker_prod/#1-builder-stage-image","title":"1. Builder Stage Image","text":"<p>Uses the <code>uv</code> image to build the application and install dependencies including the packaged project application.</p> <pre><code>FROM ghcr.io/astral-sh/uv:$UV_VER AS builder\nENV UV_COMPILE_BYTECODE=1 UV_LINK_MODE=copy\nWORKDIR /${WORKSPACE_NAME}\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --frozen --no-install-workspace --no-dev\nCOPY . /${WORKSPACE_NAME}\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --frozen --all-packages --no-dev\n</code></pre>"},{"location":"guides/docker_prod/#2-final-image","title":"2. Final Image","text":"<p>Uses a slim Python image for the runtime environment and copies the built application from the builder stage.</p> <pre><code>FROM python:3.12-slim-bookworm\nCOPY --from=builder --chown=${WORKSPACE_NAME}:${WORKSPACE_NAME} /${WORKSPACE_NAME} /${WORKSPACE_NAME}\nENV PATH=\"/${WORKSPACE_NAME}/.venv/bin:$PATH\"\nCMD [\"uv\", \"run\", \"hello\"]\n</code></pre>"},{"location":"guides/docker_prod/#docker-composeyml-explained","title":"<code>docker-compose.yml</code> Explained","text":"<p>The <code>docker-compose.yml</code> file defines the services, networks, and volumes for the application. Here's a breakdown of the key sections:</p>"},{"location":"guides/docker_prod/#build-arguments","title":"Build Arguments","text":"<p>Defines reusable build arguments for the Dockerfile.</p> <pre><code>x-args: &amp;default-args\n  WORKSPACE_NAME: \"app\"\n  UV_VER: \"python3.12-bookworm\"\n</code></pre>"},{"location":"guides/docker_prod/#1-standard-services","title":"1. Standard Services","text":"<p>Builds and runs the application using the standard Dockerfile.</p> <ul> <li>Defines the <code>app</code> service, which is built from the <code>Dockerfile</code> in the current directory.</li> <li>The <code>args</code> section passes build arguments to the <code>Dockerfile</code>.</li> <li>The <code>ports</code> section maps port 8000 on the host to port 8000 on the container.</li> <li>The <code>command</code> section specifies the command to run when the container starts.</li> </ul> <pre><code>services:\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n      args:\n        &lt;&lt;: *default-args\n    image: lit_autoencoder_app\n    ports:\n      - \"8000:8000\"\n    command: [\"uv\", \"run\", \"hello\"]\n</code></pre>"},{"location":"guides/docker_prod/#2-optimized-docker-service","title":"2. Optimized Docker Service","text":"<p>Builds and runs the application using the multi-stage Dockerfile.</p> <ul> <li>Defines the <code>app-optimized-docker</code> service, which is built from the <code>multistage.Dockerfile</code> in the current directory.</li> <li>The <code>args</code> section passes build arguments to the <code>multistage.Dockerfile</code>.</li> <li>The <code>ports</code> section maps port 8000 on the host to port 8000 on the container.</li> <li>The <code>command</code> section specifies the command to run when the container starts.</li> </ul> <pre><code>services:\n app-optimized-docker:\n    build:\n      context: .\n      dockerfile: multistage.Dockerfile\n      args:\n        &lt;&lt;: *default-args\n    image: lit_autoencoder_app\n    ports:\n      - \"8000:8000\"\n    command: [\"uv\", \"run\", \"hello\"]\n</code></pre>"},{"location":"guides/docker_prod/#build-and-run-and-test-the-docker-application","title":"Build and Run and Test the Docker Application","text":""},{"location":"guides/docker_prod/#build-the-docker-image-and-run-a-container","title":"Build the docker image and run a container","text":"<p>Build and run a specific or all services when multiple services (\"app\" and \"app-optimized-docker\") are defined in <code>docker-compose.yml</code>. Note that in the give example both services us the same port and only one service at a time should be used.</p> <pre><code>docker-compose up --build\n</code></pre> <p>or to build a single service only \"app\" respectively \"app-optimized-docker\".</p> <pre><code>docker-compose up --build app\n</code></pre> <pre><code>docker-compose up --build app-optimized-docker\n</code></pre>"},{"location":"guides/docker_prod/#test-the-endpoint-with-curl","title":"Test the endpoint with curl","text":"<ul> <li> <p>Welcome root endpoint</p> <pre><code>curl -X GET http://0.0.0.0:8000/\n</code></pre> </li> <li> <p>Get docs of the request options of the FastAPI app:</p> <pre><code>curl -X GET http://0.0.0.0:8000/docs\n</code></pre> </li> <li> <p>Test the endpoint with curl by training the model first, followed by requesting predictions for n fake images</p> <pre><code>curl -X POST http://0.0.0.0:8000/train \\\ncurl -X POST http://0.0.0.0:8000/embed -H \"Content-Type: application/json\" -d '{\"n_fake_images\": 4}'\n</code></pre> </li> </ul>"},{"location":"guides/docker_prod/#best-practices","title":"Best Practices","text":"<ul> <li>Use Multi-Stage Builds: Multi-stage builds help to reduce the final image size by only including the necessary artifacts.</li> <li>Use <code>.dockerignore</code>: Exclude unnecessary files and directories from the Docker build context to improve build performance and reduce image size.</li> <li>Minimize Layers: Combine multiple commands into a single <code>RUN</code> command to reduce the number of layers in the image.</li> <li>Use Non-Root User: Run the application as a non-root user to improve security.</li> <li>Use Environment Variables: Use environment variables to configure the application at runtime.</li> <li>Use a Volume: Use a volume to persist data between container restarts.</li> </ul>"},{"location":"guides/docker_prod/#getting-started","title":"Getting Started","text":"<ol> <li>Install Docker Compose: Ensure that Docker Compose is installed on your system. It often comes bundled with Docker Desktop for Mac and Windows.</li> <li>Clone the Repository: Clone the project repository to your local machine, so you have access to the Docker configuration files.</li> <li>Build the Docker Image: Build the Docker image using the provided <code>Dockerfile</code> or <code>multistage.Dockerfile</code>. To build (or rebuild) your service, use: <code>docker-compose build</code></li> <li>Run the Application: Start the application using Docker Compose. To start your service, use: <code>docker-compose up</code></li> <li>Access the Application: Open your web browser and go to <code>http://localhost:8000</code> to view the running application.</li> </ol>"},{"location":"guides/docker_prod/#conclusion","title":"Conclusion","text":"<p>This document provides a comprehensive overview of the Docker production setup used in this project. By following the guidelines and best practices outlined in this document, you can create a lean, efficient, and secure production environment for your application.</p>"},{"location":"guides/docker_vscode_devcontainer/","title":"Docker-VSCode-DevContainer","text":""},{"location":"guides/docker_vscode_devcontainer/#vscode-dev-container-setup-for-data-science-projects-using-uv","title":"VSCode Dev-Container Setup for Data Science Projects using UV","text":"<p>This is a containerized dev setup respectively related for using remote containers to work on data science / machine learning projects using UV, Git with VSCode.</p> <p>The repository contains a setup of a local development container using docker compose and VS Code to develop data science projects with UV in a consistent and robust but yet in a simple and customizable way. The Repo provides the configurations and installations for your container, so you can straight get started with your data science work while enjoying the benefits of using docker containers.</p> <p>Find the Documentation about this VSCode Dev-Container for Data Science.</p> <ul> <li>VSCode Dev-Container Setup for Data Science Projects using UV</li> <li>Dev Container Main Configurations and Installations<ul> <li>Setup organization within: Dockerfile.debug | docker-compose.yml | devcontainer.json</li> </ul> </li> <li>Getting Started</li> <li>VS Code Extensions for Data Science Projects<ul> <li>Additional python packages for development</li> <li>Additional python packages for project and code documentation</li> <li>Additional tools to support development</li> </ul> </li> </ul>"},{"location":"guides/docker_vscode_devcontainer/#dev-container-main-configurations-and-installations","title":"Dev Container Main Configurations and Installations","text":"<p>Using a Python image with UV pre-installed, which includes Python, as well as a Debian base image including the VS Code devcontainer base image. Multi-Stage Build: Despite using two base images, only one container is built. The final container is based on the second image, while copying content from the UV stage into the final stage.</p> <ul> <li>UV, an extremely fast Python package, virtual environment and project manager.<ul> <li>\ud83d\ude80 A single tool to replace pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv, and more.</li> <li>\u26a1\ufe0f 10-100x faster than pip.</li> <li>\ud83d\udc0d Installs and manages Python versions.</li> <li>\ud83d\udee0\ufe0f Runs and installs Python applications.</li> <li>\u2747\ufe0f Runs single-file scripts, with support for inline dependency metadata.</li> <li>\ud83d\uddc2\ufe0f Provides comprehensive project management, with a universal lockfile.</li> <li>\ud83d\udd29 Includes a pip-compatible interface for a performance boost with a familiar CLI.</li> <li>\ud83c\udfe2 Supports Cargo-style workspaces for scalable projects.</li> </ul> </li> <li>Volume Mapping: A volume will be used to map a directory on your local file system to a directory inside the Docker container. This way, any changes you make to your code locally will be immediately reflected inside the container, where you can run and test the code.</li> <li>Git: A distributed version control system that tracks changes in any set of computer files, usually used for coordinating work among programmers who are collaboratively developing source code during software development.</li> <li>VS Code, including extensions like Python, Jupyter Notebooks, Docker, PyLance, Ruff and more. Find my list of VS Code Extensions for Data Science Projects in this section below.</li> </ul>"},{"location":"guides/docker_vscode_devcontainer/#setup-organization-within-dockerfiledebug-docker-composeyml-devcontainerjson","title":"Setup organization within: Dockerfile.debug | docker-compose.yml | devcontainer.json","text":"<p>Those files define the dev container setup for data science projects. The list below gives a short overview of the single files. Details can be found in the comments in the single files.</p> <ul> <li> <p>Dockerfile.debug: This file defines the base image for your development container and the steps needed to configure it. Think of it as the blueprint for your container's operating system, dependencies, and tools. It's the foundation upon which everything else is built.</p> Click to toggle contents of the <code>Dockerfile</code> <pre><code># Dockerfile for development purposes.\n# ------------------------------------\n# Use a python image with uv pre-installed and a Debian base image including the VS Code devcontainer base image.\n# To use the image without VS CODE IDE, add lines as indicated (adjust docker-compose.yml as well as documented).\n# ---------------------------------\n\n# Define a build-time argument with a default value for base container images.\nARG UV_VER=0.5.24\nARG DEBIAN_VER=bookworm\nARG WORKSPACE_NAME_=workspace\nARG PROJECT_NAME_=${PROJECT_NAME}\n\n# Multi-Stage Build: Despite using two base images, only one container is built and run.\n# The final container is based on the second image, while copying content from the uv stage into the final stage.\n# FROM ghcr.io/astral-sh/uv:python3.12-bookworm\nFROM ghcr.io/astral-sh/uv:$UV_VER AS uv\n\nFROM mcr.microsoft.com/vscode/devcontainers/base:$DEBIAN_VER\n\n# Install/Update linux packages; install common dev tools like: git, process tools, ...\n# hadolint ignore=DL3008\nRUN apt-get update \\\n    &amp;&amp; apt-get install -y --no-install-recommends\\\n    procps \\\n    build-essential \\\n    curl \\\n    swig \\\n    wget \\\n    # To reduce the image size, it is recommended refresh the package cache as follows.\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copies files or directories from the uv stage into the final stage,\n# and ensures that the ownership of the copied files is adjusted to the user and group in the final image,\n# and making its functionality or binaries available in the final container.\nCOPY --from=uv --chown=vscode: /uv /uvx /bin/\n\nWORKDIR /vscode/${WORKSPACE_NAME_}\n\n# The code to run when container is started:\n# Common practice to keep the Docker container running without performing any significant action.\nENTRYPOINT [\"tail\", \"-f\", \"/dev/null\"]\n</code></pre> </li> <li> <p>docker-compose.yml: (Optional, but often used) This file is used when you have multiple containers that need to work together for your development environment. It defines how these containers interact, their dependencies, and their network configuration. Even if only a single service is used, it defines the additional configuration specific for your data science projects like volume mapping or ports which is optional and therefore not defined in the Dockerfile.debug. This enables the dev container setup to be used with other IDEs then VS Code.</p> Click to toggle contents of the <code>docker-compose.yml</code> <pre><code># Dev Container Configuration File.\n# ---------------------------------\n# Standard Configuration for the service to be used to develop data science applications.\n# Using bind mounts instead of watch for development to sync changes made in the container back to the host.\n# This file depends on a .env file in the root directory of the dev container for dynamic variable interpolation.\n# - The .env file is automatically loaded per default by Docker Compose and is not passed to the container during build.\n# ---------------------------------\n\n# The x-args section defines a reusable set of arguments using YAML anchors.\n# - BUILD arguments (\"UV_VER\", \"DEBIAN_VER\" and \"WORKSPACE_NAME\") to pass to Dockerfile.\nx-args: &amp;default-args\n  UV_VER: \"0.5.5\"\n  DEBIAN_VER: \"bookworm\"\n  WORKSPACE_NAME_: ${WORKSPACE_NAME}\n  PROJECT_NAME_: ${PROJECT_NAME}\n\nservices: # Top level element to configure the arguments of multiple services.\n  myproject: # \"project\" refers to the name of your project/application for which configurations are defined.\n    build: # Tells Docker Compose to build the Docker image using the Dockerfile in the specified directory.\n      context: .\n      dockerfile: ./Dockerfile.debug\n      # Build argument (passed to Dockerfile only)\n      args:\n        &lt;&lt;: *default-args # The &lt;&lt;: *default-args syntax merges the default-args into the args section of the build configuration.\n    image: \"${DEV_USER}.dev-container-uv.${PROJECT_NAME}\" # Explicit way to define the image name\n\n    # Host the FastAPI application on port 8000.\n    ports:\n      - \"8000:8000\"\n\n    # Volumes are persistent data stores (outside container), mounted to be usable by the container.\n    volumes:\n      # Mount the current directory to ${WORKSPACE_NAME} so code changes don't require an image rebuild. .venv is excluded in the .dockerignore file.\n      - type: bind\n        source: ..\n        target: /vscode/${WORKSPACE_NAME}\n      # Mount the virtual environment separately, so the developer's environment doesn't end up in the container.\n      - type: volume\n        source: venv\n        target: /vscode/${WORKSPACE_NAME}/.venv\n\n    working_dir: /vscode/${WORKSPACE_NAME}\n\n    # Runtime environment variable, passed to devcontainer.json. Not available for volumes, networks, or build arguments.\n    # Since docker detects a .env file during build per default, the .env file will be loaded anyways.\n    # Set explicitly for clarity to indicate that the environment variables are used.\n    env_file:\n      - path: .env\n        required: true\n\n    # Default command to start the dev container.\n    command:\n      - sh -c \"chmod -R 777 /vscode/${WORKSPACE_NAME} &amp;&amp; tail -f /dev/null\" # Set permissions on the working directory for root user.\n      - docker-compose down # Remove the container after exiting.\n\n# Define the volumes of the docker container.\nvolumes:\n  venv: # Volume for the virtual environment for persistent in the container.\n</code></pre> </li> <li> <p>devcontainer.json: It configures VS Code (or other supporting IDEs) to use the Docker container as your development environment. It links the Dockerfile (or docker-compose.yml) to VS Code and defines various settings like VS Code extensions or the default environment used by VS Code that are only related to use the given IDE.</p> Click to toggle contents of the <code>devcontainer.json</code> <pre><code>// UV | VS Code - Setup.\n//---------------------\n// This config is set up to be only specific to VS Code.\n// Other configs that do not relate to VS Code are defined in the docker-compose.yml file.\n// This enables the dev container setup to be used with other IDEs, ignoring this file.\n//---------------------\n// Default and dynamic properties for the devcontainer setup:\n// - Default service: \"myproject\", relates to the service defined in the docker-compose.yml.\n// - Default \"workspaceFolder\" is set to \"workspace\" (within the \"/vscode/\" folder in the container).\n// For format details, see https://aka.ms/devcontainer.json.\n//---------------------\n\n{\n  \"name\": \"${localEnv:LOGNAME}.dev-container-uv.${localWorkspaceFolderBasename}\",\n  // Build image using docker compose based on build specs in docker-compose.yml\n  \"dockerComposeFile\": [\"./docker-compose.yml\"],\n  \"service\": \"myproject\",\n  \"runServices\": [\"myproject\"],\n  \"workspaceFolder\": \"/vscode/workspace\",\n  \"postCreateCommand\": {\n    \"uv-sync--frozen\": \"uv sync --frozen --all-groups --all-packages --no-binary\" // By default, uv installs projects and workspace members in editable mode, such that changes to the source code are immediately reflected in the environment.\n  },\n  \"postStartCommand\": {\n    // Optional: If applicable, add the following lines for installations.\n    \"uv-run-pre-commit-install\": \"uv run pre-commit install\"\n  },\n  \"features\": {\n        \"ghcr.io/dhoeric/features/hadolint:1\": {}\n    },\n  \"customizations\": {\n    // When connecting to a docker container your local VS Code starts an instance without extensions to ensure isolation and consistency.\n    // Therefore extensions can be specified here for automatic installation when connecting.\n    \"vscode\": {\n      \"settings\": {\n        // Define terminal shell for Dev Container.\n        \"terminal.integrated.profiles.linux\": {\n          \"bash\": {\n            \"path\": \"/bin/bash\"\n          }\n        },\n        \"jupyter.notebookFileRoot\": \"${workspaceRoot}\",\n        \"python.pythonPath\": \"/home/vscode/workspace/.venv/bin/python\"\n      },\n      // Use the VS Code Extensions \"Identifier\" to define extensions.\n      \"extensions\": [\n        // Python\n        \"ms-python.python\",\n        \"ms-toolsai.jupyter\",\n        // Docker\n        \"ms-azuretools.vscode-docker\",\n        \"ms-vscode-remote.remote-containers\",\n        \"ms-vscode-remote.remote-ssh-edit\",\n        \"ms-vscode-remote.remote-ssh\",\n        \"exiasr.hadolint\",\n        // Formatting and Linting\n        \"charliermarsh.ruff\",\n        \"davidanson.vscode-markdownlint\",\n        \"xshrim.txt-syntax\",\n        \"tamasfe.even-better-toml\",\n        \"streetsidesoftware.code-spell-checker\",\n        \"ms-pyright.pyright\",\n        \"ms-python.vscode-pylance\",\n        // Data\n        \"alexcvzz.vscode-sqlite\",\n        \"grapecity.gc-excelviewer\",\n        \"mechatroner.rainbow-csv\",\n        \"zainchen.json\",\n        \"yzane.markdown-pdf\",\n        \"ms-toolsai.datawrangler\",\n        \"yzhang.markdown-all-in-one\",\n        // Cloud\n        //\"ms-azuretools.vscode-docker\",\n        // Git\n        \"github.remotehub\",\n        // AI Coding Assistant\n        \"github.copilot\",\n        \"github.copilot-chat\",\n        // Other\n        \"richie5um2.vscode-sort-json\",\n        \"oliversen.chatgpt-docstrings\"\n      ]\n    }\n  },\n  \"remoteUser\": \"vscode\"\n}\n</code></pre> </li> </ul>"},{"location":"guides/docker_vscode_devcontainer/#getting-started","title":"Getting Started","text":"<ul> <li>Install Docker Desktop.</li> <li>Install VSCode.</li> <li>Install VSCode Extensions.<ul> <li>Install VSCode remote container extension: ms-vscode-remote.remote-containers.</li> </ul> </li> <li>Start dev container:  F1 + \"Open folder in container ...\".</li> <li> <p>To try the setup:</p> <ul> <li>Clone the repository.</li> <li>Run demo.py in the Python virtual environment (used as default, see <code>devcontainer.json</code>).</li> <li> <p>Your application will be available at http://localhost:8000.</p> <ul> <li> <p>Welcome root request of the FastAPI app, providing an app description.</p> <pre><code>curl -X GET http://localhost:8000/\n</code></pre> </li> <li> <p>Test the machine learning endpoints with curl:</p> <pre><code>curl -X POST http://localhost:8000/train \\\ncurl -X POST http://localhost:8000/embed -H \"Content-Type: application/json\" -d '{\"n_fake_images\": 1}'\n</code></pre> </li> </ul> </li> <li> <p>For more information about the application, see the README in the root directory of the repository.</p> </li> </ul> </li> </ul> <p>Optional Steps:</p> <ul> <li>Change the Docker Container in Dockerfile and/or docker-compose.yml.</li> <li>Add/remove VSCode settings and extensions in .devcontainer/devcontainer.json.</li> <li>Update .gitignore and other config files to match your setup.</li> </ul>"},{"location":"guides/docker_vscode_devcontainer/#vs-code-extensions-for-data-science-projects","title":"VS Code Extensions for Data Science Projects","text":"<p>The below extensions are installed for VS Code in the dev container, as defined in <code>devcontainer.json</code>.</p> <pre><code>alexcvzz.vscode-sqlite # SQLite query execution and browsing\ncharliermarsh.ruff # Python linter and code formatter\ndavidanson.vscode-markdownlint # Markdown linter\nexiasr.hadolint # A Dockerfile linter\ngithub.copilot # AI-powered code completion\ngithub.copilot-chat # Chat with your AI coding assistant\ngithub.remotehub # Manage your GitHub repositories\ngrapecity.gc-excelviewer # Excel file viewer\nhediet.vscode-drawio # Draw.io integration\njeff-hykin.polacode-2019 # Generate code blocks from images\nmechatroner.rainbow-csv # Colorizes CSV files for better readability\nms-azuretools.vscode-docker # Docker extension for VS Code\nms-pyright.pyright # Static type checker\nms-python.python # Python language support\nms-python.vscode-pylance # Python language server for enhanced code analysis\nms-toolsai.datawrangler # Data transformation and cleaning tool\nms-toolsai.jupyter # Jupyter Notebooks\nms-pyright.pyright # Static type checker\noliversen.chatgpt-docstrings\nrichie5um2.vscode-sort-json # Sorts JSON objects\nstreetsidesoftware.code-spell-checker # Spell checker for multiple languages\ntamasfe.even-better-toml # Enhanced TOML language support\ntomoki1207.pdf # PDF viewer\nxshrim.txt-syntax # Syntax highlighting for plain text files\nzainchen.json # JSON language support (likely provides syntax highlighting, validation, etc.)\nyzane.markdown-pdf # Generate PDF from Markdown files\nyzhang.markdown-all-in-one # All-in-one Markdown extension\n</code></pre>"},{"location":"guides/docker_vscode_devcontainer/#additional-python-packages-for-development","title":"Additional python packages for development","text":"<ul> <li>uv</li> <li>ruff</li> <li>pytest</li> <li>pytest-cov</li> <li>pre-commit</li> <li>pyright</li> <li>ipykernel</li> <li>jupyterlab</li> <li>toml-sort</li> </ul>"},{"location":"guides/docker_vscode_devcontainer/#additional-python-packages-for-project-and-code-documentation","title":"Additional python packages for project and code documentation","text":"<ul> <li>mkdocs</li> <li>mkdocs-include-markdown-plugin</li> <li>mkdocs-jupyter</li> <li>mkdocs-material</li> <li>mkdocstrings[python]</li> </ul>"},{"location":"guides/docker_vscode_devcontainer/#additional-tools-to-support-development","title":"Additional tools to support development","text":"<ul> <li>GitHub</li> <li>GitHub Actions and Workflows for CI</li> <li>GitHub Pages (to host your documentation)</li> </ul>"},{"location":"guides/mkdocs/","title":"MkDocs Documentation","text":"<p>MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file.</p> <ul> <li>MkDocs Documentation</li> <li>Installation</li> <li>Configuration</li> <li>Using mkdocstrings</li> <li>Using mkdocs-jupyter</li> <li>Create Documentation<ul> <li>Init Documentation</li> <li>Create MkDocs Configuration File</li> <li>Generate General Documentation</li> <li>Generate API Documentation</li> </ul> </li> <li>Building the Documentation</li> <li>Serving the Documentation</li> <li>Deploying the Documentation</li> </ul>"},{"location":"guides/mkdocs/#installation","title":"Installation","text":"<p>Install MkDocs and the related extensions (using uv):</p> <ul> <li>mkdocs</li> <li>mkdocs-include-markdown-plugin</li> <li>mkdocs-material</li> <li>mkdocstrings</li> <li>mkdocs-jupyter</li> </ul>"},{"location":"guides/mkdocs/#configuration","title":"Configuration","text":"<p>The <code>mkdocs.yml</code> file is the configuration file for MkDocs. It contains settings such as the site name, navigation, theme, and plugins.</p> <p>Key Configuration Options Used in this Project:</p> <ul> <li><code>site_name</code>: The name of the site.</li> <li><code>nav</code>: The navigation structure of the site. This defines the table of contents.</li> <li><code>theme</code>: The theme used for the site. We use the material theme.</li> <li><code>plugins</code>: A list of plugins used by MkDocs.<ul> <li><code>search</code>: The built-in search plugin adds a search box to the site.</li> <li><code>include-markdown</code>: This plugin allows you to include Markdown files within other Markdown files. This is useful for reusing content across multiple pages.</li> <li><code>mkdocstrings</code>: This plugin generates documentation from Python docstrings. It is used to generate the \"Source Code API Reference\" section in the navigation.</li> <li><code>mkdocs-jupyter</code>: This plugin allows you to include Jupyter notebooks in your MkDocs documentation.</li> </ul> </li> <li><code>extra_css</code>:<ul> <li><code>stylesheets/extra.css</code>: Custom CSS file to style the documentation.</li> </ul> </li> </ul>"},{"location":"guides/mkdocs/#using-mkdocstrings","title":"Using mkdocstrings","text":"<p>The mkdocstrings plugin is configured to use the google docstring style. This means that the plugin will parse docstrings that follow the Google docstring format. For example:</p> <pre><code>def my_function(arg1, arg2):\n    \"\"\"\n    This is a function that does something.\n\n    Args:\n        arg1: The first argument.\n        arg2: The second argument.\n\n    Returns:\n        The result of the function.\n    \"\"\"\n    return arg1 + arg2\n</code></pre> <p>The mkdocstrings plugin will automatically generate documentation for this function, including the arguments, return value, and description. The \"Source Code API Reference\" section in the navigation is generated using this plugin.</p>"},{"location":"guides/mkdocs/#using-mkdocs-jupyter","title":"Using mkdocs-jupyter","text":"<p>The mkdocs-jupyter plugin allows you to include Jupyter notebooks in your MkDocs documentation. To include a notebook, simply add it to the nav section of your mkdocs.yml file. The plugin will automatically convert the notebook to HTML and include it in your documentation. This is very useful for tutorials. For example:</p> <pre><code>nav:\n  - Notebook: notebook.ipynb\n</code></pre>"},{"location":"guides/mkdocs/#create-documentation","title":"Create Documentation","text":""},{"location":"guides/mkdocs/#init-documentation","title":"Init Documentation","text":"<p>Init documentation using <code>mkdocs</code> for an existing project:</p> <pre><code>uv run mkdocs new .\n</code></pre> <p>This will create a mkdocs.yml file and a docs/ folder with an index.md file.</p> <pre><code>project/\n\u2502\n\u251c\u2500\u2500 src/calculator/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 calculations.py\n\u2502\n\u251c\u2500\u2500 docs/\n\u2502   \u2514\u2500\u2500 index.md\n\u2502\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"guides/mkdocs/#create-mkdocs-configuration-file","title":"Create MkDocs Configuration File","text":"<p>Create\u00a0mkdocs\u00a0Configuration File: Edit the\u00a0mkdocs.yml\u00a0file in the root of your project. This file will contain the configuration for your\u00a0mkdocs\u00a0site.</p> <pre><code>site_name: UV Data Science Project Template\nnav:\n  - Home: index.md\n  - Guides:\n    - Ruff: guides/ruff.md\n    - UV: guides/uv.md\n    - PyTest: guides/pytest.md\n    - PyRight: guides/pyright.md\n    - Pre-Commit: guides/pre-commit.md\n    - MkDocs: guides/mkdocs.md\n    - Docker-Production: guides/docker_prod.md\n    - Docker-VSCode-DevContainer: guides/docker_vscode_devcontainer.md\n    - CI-GitHub: guides/ci_github.md\n  - Source Code API Reference:\n      - Autoencoder: api/autoencoder.md\n      - Training: api/training.md\n      - FastAPI: api/fastapi_app.md\ntheme:\n  name: material\nplugins:\n  - search\n  - include-markdown\n  - mkdocstrings:\n      handlers:\n        python:\n          options:\n            docstring_style: google\n</code></pre>"},{"location":"guides/mkdocs/#generate-general-documentation","title":"Generate General Documentation","text":"<p>Generate General Documentation: For example use the <code>index.md</code> to reference your <code>README.md</code> file. <code>docs/index.md</code>:</p> <pre><code># Dev Container UV Data Science\n\nWelcome to the documentation for the Dev Container UV Data Science project.\n\n&lt;!-- Include the content of README.devcontainer.md --&gt;\n&lt;!-- Using PyMdown Snippets for inclusion relative to the base_path defined in the mkdocs.yml--&gt;\n&lt;!-- Uncomment the following lines to include the README.devcontainer.md file --&gt;\n&lt;!-- --8&lt;-- \".devcontainer/README.devcontainer.md\" --&gt;\n</code></pre>"},{"location":"guides/mkdocs/#generate-api-documentation","title":"Generate API Documentation","text":"<p>Use\u00a0<code>mkdocstrings</code>\u00a0to generate API documentation from your Python docstrings. In your\u00a0<code>autoencoder.md</code>\u00a0and\u00a0<code>training.md</code>\u00a0files, you can include references to your Python modules:</p> <ul> <li> <p>Create Documentation Files: Create a\u00a0docs\u00a0directory in the root of your project. Inside this directory, create the necessary Markdown files for your documentation.</p> <pre><code>mkdir docs/api\ntouch docs/api/autoencoder.md\ntouch docs/api/training.md\n</code></pre> </li> </ul> <p><code>docs/docs/autoencoder.md</code>:</p> <pre><code># Autoencoder\n\n::: dev_container_uv_datascience.lit_auto_encoder\n</code></pre> <p><code>docs/training.md</code>:</p> <pre><code># Training\n\n::: dev_container_uv_datascience.train_autoencoder\n</code></pre>"},{"location":"guides/mkdocs/#building-the-documentation","title":"Building the Documentation","text":"<p>To build the documentation, run the command below. This will generate a static site in the site directory.</p> <pre><code>uv run mkdocs build\n</code></pre>"},{"location":"guides/mkdocs/#serving-the-documentation","title":"Serving the Documentation","text":"<p>To serve the documentation locally, run the command below. This will start a local web server that you can use to view the documentation.</p> <pre><code>uv run mkdocs serve\n</code></pre>"},{"location":"guides/mkdocs/#deploying-the-documentation","title":"Deploying the Documentation","text":"<p>The documentation can be deployed to a variety of platforms, such as GitHub Pages, Netlify, or AWS S3. This project is set up to deploy to GitHub Pages using GitHub Actions. See the CI-GitHub guide for more information.</p> <p>To deploy your documentation, you can use the\u00a0<code>mkdocs gh-deploy</code>\u00a0command to deploy it to GitHub Pages:</p> <pre><code>uv run mkdocs gh-deploy\n</code></pre> <p>Ensure you enable GitHub pages via the repo settings, see:</p> <ul> <li>GitHub - Deploy MkDocs</li> <li>MkDocs - Deploying your docs</li> <li>Publishing your site</li> <li>GitHub Pages</li> </ul>"},{"location":"guides/pre_commit/","title":"Using Pre-Commit Hooks","text":"<p>Pre-commit hooks are scripts that run automatically before each commit. They help to identify and fix issues before they are integrated into the codebase, ensuring code quality and consistency.</p> <ul> <li>Using Pre-Commit Hooks</li> <li>Benefits of Using Pre-Commit Hooks?</li> <li>How to Use Pre-Commit Hooks in This Project<ul> <li>1. Installation</li> <li>2. Install Pre-Commit Hooks and/or Running Hooks Manually</li> <li>3. Configuring Hooks</li> <li>4. Customizing Hooks</li> </ul> </li> </ul>"},{"location":"guides/pre_commit/#benefits-of-using-pre-commit-hooks","title":"Benefits of Using Pre-Commit Hooks?","text":"<p>Pre-commit hooks automate code checks, formatting, and other tasks, saving time and effort during code reviews. They catch common problems early, preventing them from becoming bigger issues later.</p>"},{"location":"guides/pre_commit/#how-to-use-pre-commit-hooks-in-this-project","title":"How to Use Pre-Commit Hooks in This Project","text":"<p>This project uses pre-commit to manage pre-commit hooks. Here's how to get started:</p>"},{"location":"guides/pre_commit/#1-installation","title":"1. Installation","text":"<p>Make sure you have <code>uv</code> installed. If not, refer to the installation guide.</p>"},{"location":"guides/pre_commit/#2-install-pre-commit-hooks-andor-running-hooks-manually","title":"2. Install Pre-Commit Hooks and/or Running Hooks Manually","text":"<ul> <li> <p>Install Pre-Commit Hooks to automatically when you commit changes. Run the following command to install the pre-commit hooks:</p> <pre><code>uv run pre-commit install\n</code></pre> </li> <li> <p>Running Hooks Manually If you want to run the hooks manually, use the following command:</p> <pre><code>uv run pre-commit run --all-files\n</code></pre> </li> </ul>"},{"location":"guides/pre_commit/#3-configuring-hooks","title":"3. Configuring Hooks","text":"<p>The pre-commit hooks are configured in the <code>.pre-commit-config.yaml</code> file. This file specifies the hooks to use, their settings, and other options.</p> <p>Here's a breakdown of the hooks used in this project:</p> <ul> <li>Ruff: A fast Python linter and formatter. It checks for code style issues and automatically fixes them. Refer to the <code>ruff.toml</code> file for the configurations.<ul> <li>Ruff Check: Runs the Ruff linter to identify issues. The <code>--fix</code> argument automatically fixes fixable issues. <code>--exit-non-zero-after-fix</code> will cause the hook to fail if ruff makes changes.</li> <li>Ruff Format: Runs the Ruff formatter to format the code. The <code>--diff</code> argument displays a diff of the changes instead of applying them directly.</li> </ul> </li> <li>Pyright: A static type checker for Python. It checks for type errors in the code. Refer to the <code>pyrightconfig.json</code> file for the configurations.</li> <li>Hadolint: A linter for Dockerfiles. It checks for common issues and best practices in Dockerfiles. The <code>--config .hadolint.yaml</code> argument specifies a configuration file for Hadolint.</li> </ul>"},{"location":"guides/pre_commit/#4-customizing-hooks","title":"4. Customizing Hooks","text":"<p>You can customize the pre-commit hooks by modifying the <code>.pre-commit-config.yaml</code> file. You can add new hooks, remove existing hooks, or change the settings of existing hooks.</p> <p>For more information on how to configure pre-commit hooks, see the pre-commit documentation.</p>"},{"location":"guides/pyright/","title":"Pyright Usage Guide","text":"<p>Pyright is a static type checker for Python. It helps you catch type-related errors early in the development process, improving the reliability and maintainability of your code. This guide explains how to use Pyright in this project, including configuration and best practices.</p> <ul> <li>Pyright Usage Guide</li> <li>Installation</li> <li>Configuration</li> <li>Usage<ul> <li>VS Code Integration</li> <li>Command-Line Usage</li> <li>Ignoring Errors</li> </ul> </li> <li>Integration with Ruff</li> <li>Best Practices</li> </ul>"},{"location":"guides/pyright/#installation","title":"Installation","text":"<p>Pyright is included as a development dependency in this project. You can install it using <code>uv</code>:</p> <pre><code>uv add pyright\n</code></pre>"},{"location":"guides/pyright/#configuration","title":"Configuration","text":"<p>Pyright is configured using the <code>pyrightconfig.json</code> file in the root of the project. Here's a breakdown of the configuration options:</p> <ul> <li><code>pythonVersion</code>: Specifies the Python version used for type checking.</li> <li><code>pythonPlatform</code>: Specifies the target platform.</li> <li><code>venvPath</code>: Specifies the path to the virtual environment.</li> <li><code>typeCheckingMode</code>: Specifies the type checking mode. Options include <code>\"basic\"</code>, <code>\"strict\"</code>, and <code>\"standard\"</code>.</li> <li><code>include</code>: A list of directories to include in type checking.</li> <li><code>exclude</code>: A list of directories to exclude from type checking.</li> </ul> <pre><code>{\n    \"pythonVersion\": \"3.12\",\n    \"pythonPlatform\": \"All\",\n    \"venvPath\": \"./.venv\",\n    \"typeCheckingMode\": \"standard\",\n    \"include\": [\n        \"src/\",\n        \"tests/\"\n    ],\n    \"exclude\": [\n        \"**/__pycache__\",\n        \".pytest_cache\",\n        \".ruff_cache\",\n        \".venv\"\n    ]\n}\n</code></pre>"},{"location":"guides/pyright/#usage","title":"Usage","text":"<p>To run Pyright, use the following command:</p> <pre><code>uv run pyright\n</code></pre> <p>This will analyze the code in the <code>src/</code> and <code>tests/</code> directories based on the configuration in <code>pyrightconfig.json</code>.</p>"},{"location":"guides/pyright/#vs-code-integration","title":"VS Code Integration","text":"<p>For VS Code, the Pylance extension provides excellent integration with Pyright. It offers real-time type checking and code completion as you type.</p> <p>To enable Pylance, install the extension and ensure that it is configured to use the project's Python environment.</p>"},{"location":"guides/pyright/#command-line-usage","title":"Command-Line Usage","text":"<p>You can also run Pyright from the command line using the <code>pyright</code> command. This is useful for CI/CD pipelines and pre-commit hooks.</p> <pre><code>uv run pyright\n</code></pre>"},{"location":"guides/pyright/#ignoring-errors","title":"Ignoring Errors","text":"<p>Sometimes, you may want to ignore specific Pyright errors. You can do this using the <code># pyright: ignore</code> comment.</p> <pre><code>x = 1  # pyright: ignore\n</code></pre> <p>You can also ignore specific error codes:</p> <pre><code>x = 1  # pyright: ignore[reportGeneralTypeIssues]\n</code></pre>"},{"location":"guides/pyright/#integration-with-ruff","title":"Integration with Ruff","text":"<p>This project uses Ruff for linting and formatting. Ruff can also run Pyright as part of its checks. To enable this, configure Ruff to include Pyright rules. However, in this project, we run pyright separately.</p>"},{"location":"guides/pyright/#best-practices","title":"Best Practices","text":"<ul> <li>Write type hints for all functions and variables.</li> <li>Use the <code>typing</code> module for advanced type hinting.</li> <li>Run Pyright regularly to catch errors early.</li> <li>Configure your editor to use Pylance for real-time type checking.</li> </ul> <p>By following these guidelines, you can ensure that your code is well-typed and maintainable.</p>"},{"location":"guides/pytest/","title":"Pytest and Coverage Guide","text":"<p>This guide explains how to use <code>pytest</code> for testing and <code>coverage.py</code> for measuring test coverage in this project.</p> <ul> <li>Pytest and Coverage Guide</li> <li>Introduction to Pytest<ul> <li>Key Features</li> </ul> </li> <li>Running Tests<ul> <li>Specific Tests</li> </ul> </li> <li>Introduction to Coverage.py<ul> <li>Coverage Key Features</li> </ul> </li> <li>Running Tests with Coverage</li> <li>Configuration<ul> <li>.coveragerc</li> <li>pytest.ini or pyproject.toml</li> </ul> </li> <li>Usage in This Project</li> <li>Example Workflow</li> </ul>"},{"location":"guides/pytest/#introduction-to-pytest","title":"Introduction to Pytest","text":"<p>pytest is a powerful and flexible testing framework for Python. It simplifies writing and running tests, and provides a rich set of features for test discovery, parametrization, fixtures, and more.</p>"},{"location":"guides/pytest/#key-features","title":"Key Features","text":"<ul> <li>Simple Syntax: Pytest uses a simple and intuitive syntax for writing tests.</li> <li>Test Discovery: It automatically discovers test files and test functions.</li> <li>Fixtures: Fixtures provide a way to set up and tear down test environments.</li> <li>Plugins: A rich ecosystem of plugins extends pytest's functionality.</li> </ul>"},{"location":"guides/pytest/#running-tests","title":"Running Tests","text":"<p>To run tests, use the following command in the project's root directory:</p> <pre><code>uv run pytest\n</code></pre> <p>This command will discover and run all files matching <code>test_*.py</code> or <code>*_test.py</code> in the project's directory and subdirectories.</p>"},{"location":"guides/pytest/#specific-tests","title":"Specific Tests","text":"<p>To run a specific test file or test function, you can specify its path:</p> <pre><code>uv run pytest tests/test_module.py\nuv run pytest tests/test_module.py::test_function\n</code></pre>"},{"location":"guides/pytest/#introduction-to-coveragepy","title":"Introduction to Coverage.py","text":"<p>coverage.py is a tool for measuring code coverage in Python. It monitors your program, notes which parts of the code have been executed, and then analyzes the source to identify code that could have been executed but was not.</p>"},{"location":"guides/pytest/#coverage-key-features","title":"Coverage Key Features","text":"<ul> <li>Line Coverage: Measures which lines of code were executed during testing.</li> <li>Branch Coverage: Measures which branches in the code were taken during testing.</li> <li>Integration with Pytest: Seamless integration with pytest for running tests and collecting coverage data.</li> <li>Reporting: Generates reports in various formats, including HTML, XML, and text.</li> </ul>"},{"location":"guides/pytest/#running-tests-with-coverage","title":"Running Tests with Coverage","text":"<p>To run tests with coverage, use the following command:</p> <pre><code>uv run pytest --cov\n</code></pre> <p>This command will run all tests and collect coverage data for the <code>src</code> directory. The <code>--cov</code> flag specifies the source directory to measure coverage for.</p>"},{"location":"guides/pytest/#configuration","title":"Configuration","text":""},{"location":"guides/pytest/#coveragerc","title":".coveragerc","text":"<p>The <code>.coveragerc</code> file configures how <code>coverage.py</code> collects and reports coverage data. Key settings include:</p> <ul> <li><code>data_file</code>: Specifies the location where coverage data is stored. In this project, it's set to <code>.test_reports/coverage/.coverage</code>.</li> <li><code>source</code>: Specifies the source directories to measure coverage for. In this project, it's set to <code>src</code>.</li> <li><code>branch</code>: Enables branch coverage.</li> <li><code>omit</code>: Specifies files to exclude from coverage reporting.</li> <li><code>report</code>: This will create an HTML report in the <code>.test_reports/coverage/html</code> directory. You can then open the <code>index.html</code> file in your browser to view the report.</li> <li>XML Report: In the CI Workflow a XML report will be created in the <code>.test_reports/coverage/coverage.xml</code> file, which can be used for integration with CI/CD systems.</li> </ul>"},{"location":"guides/pytest/#pytestini-or-pyprojecttoml","title":"pytest.ini or pyproject.toml","text":"<p>Pytest can be configured using a <code>pytest.ini</code> file or a <code>pyproject.toml</code> file. This project uses a <code>pytest.ini</code> file.</p> <ul> <li><code>testpaths</code>: Specifies the directories to search for tests.</li> <li><code>addopts</code>: Specifies command-line options to always use when running pytest.</li> <li><code>filterwarnings</code>: To handle warnings during testing.</li> </ul>"},{"location":"guides/pytest/#usage-in-this-project","title":"Usage in This Project","text":"<p>This project is set up to automatically collect coverage data when running tests. The <code>.coveragerc</code> file is configured to:</p> <ul> <li>Store coverage data in the <code>.test_reports/coverage/</code> directory.</li> <li>Measure coverage for the <code>src</code> directory.</li> <li>Generate Terminal reports in the <code>.test_reports/coverage/</code> directory.</li> <li>Generate HTML reports in the <code>.test_reports/coverage/html</code> directory.</li> </ul>"},{"location":"guides/pytest/#example-workflow","title":"Example Workflow","text":"<ol> <li>Write your code in the <code>src</code> directory.</li> <li>Write tests for your code in the <code>tests</code> directory.</li> <li>Run tests with coverage: <code>uv run pytest --cov</code></li> <li>Check the terminal report to see which parts of your code are not covered by tests.</li> <li>Write more tests to increase coverage.</li> <li>Repeat steps 3-5 until you have sufficient coverage.</li> </ol> <p>By following this guide, you can effectively use <code>pytest</code> and <code>coverage.py</code> to write and test your code, ensuring high code quality and reliability.</p>"},{"location":"guides/ruff/","title":"Ruff","text":"<p>Ruff is an extremely fast Python linter, formatter, and code assistant. It's written in Rust and is designed to be a drop-in replacement for tools like Flake8, pycodestyle, pyupgrade, isort, and others.</p> <ul> <li>Ruff</li> <li>Key Features</li> <li>Ruff in this Project<ul> <li>Configuration (<code>ruff.toml</code>)</li> <li>Usage</li> <li>Pre-commit Hooks</li> <li>Command Line</li> <li>Integrating with Editors</li> <li>Customizing Ruff</li> <li>Pydocstyle Convention</li> </ul> </li> </ul>"},{"location":"guides/ruff/#key-features","title":"Key Features","text":"<ul> <li>Extremely Fast: Written in Rust, Ruff is significantly faster than traditional Python linters.</li> <li>Comprehensive: Combines the functionality of multiple tools, including linters, formatters, and code fixers.</li> <li>Configurable: Highly customizable through the <code>ruff.toml</code> configuration file.</li> <li>Easy to Integrate: Integrates well with popular editors and CI/CD systems.</li> <li>Automatic Fixes: Can automatically fix many common linting errors.</li> </ul>"},{"location":"guides/ruff/#ruff-in-this-project","title":"Ruff in this Project","text":"<p>This project uses Ruff for linting, formatting, and ensuring code quality. The configuration is managed through the <code>ruff.toml</code> file located in the root directory of the project.</p>"},{"location":"guides/ruff/#configuration-rufftoml","title":"Configuration (<code>ruff.toml</code>)","text":"<p>The <code>ruff.toml</code> file specifies the settings for Ruff. Here's a breakdown of the key configurations:</p> <ul> <li><code>exclude</code>: Specifies directories that Ruff should ignore, such as <code>.git</code>, <code>.venv</code>, <code>dist</code>, etc.</li> <li><code>line-length</code>: Sets the maximum line length for the project (currently 99 characters).</li> <li><code>target-version</code>: Specifies the target Python version (currently Python 3.12).</li> <li><code>lint.select</code>: Defines the set of rules that Ruff should enforce. This includes:<ul> <li><code>E</code>: pycodestyle errors</li> <li><code>N</code>: pep8-naming conventions</li> <li><code>F</code>: pyflakes</li> <li><code>D</code>: pydocstyle</li> </ul> </li> <li><code>lint.ignore</code>: Specifies rules that should be ignored. This project ignores several pydocstyle rules related to missing docstrings in certain contexts.</li> <li><code>fixable</code>: Specifies rules that Ruff can automatically fix.</li> <li><code>dummy-variable-rgx</code>: Configures the regular expression for dummy variable names.</li> <li><code>format</code>: Configures code formatting options such as quote style, indentation, etc.</li> <li><code>per-file-ignores</code>: Allows specifying different configurations for different files or directories.  Tests directory ignores missing docstrings.</li> </ul>"},{"location":"guides/ruff/#usage","title":"Usage","text":"<p>Ruff is integrated into the development workflow using pre-commit hooks, the CI/CD pipelines and will be applied in auto save mode when using VS Code.</p>"},{"location":"guides/ruff/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Ruff is configured as a pre-commit hook to automatically lint and format code before each commit. This helps ensure that all code meets the project's quality standards. To use the pre-commit hooks, make sure you have pre-commit installed and configured in your local environment.</p>"},{"location":"guides/ruff/#command-line","title":"Command Line","text":"<p>Ruff can also be run from the command line:</p> <pre><code>uv run ruff check .\nuv run ruff format .\n</code></pre> <p>These commands will check and format the code in the current directory, respectively.</p>"},{"location":"guides/ruff/#integrating-with-editors","title":"Integrating with Editors","text":"<p>Ruff integrates with many popular code editors, such as VS Code, Sublime Text, and others. Refer to the Ruff documentation for editor-specific instructions.</p>"},{"location":"guides/ruff/#customizing-ruff","title":"Customizing Ruff","text":"<p>The <code>ruff.toml</code> file can be customized to suit the specific needs of the project.  Refer to the Ruff documentation for a complete list of available options.</p>"},{"location":"guides/ruff/#pydocstyle-convention","title":"Pydocstyle Convention","text":"<p>The <code>pydocstyle</code> section specifies the convention for docstrings. This project uses the \"google\" convention.</p>"},{"location":"guides/uv/","title":"UV, an extremely fast Python package and project manager, written in Rust","text":"<p>UV is used as a fast and efficient package manager for this project, replacing tools like pip, virtualenv, and pip-tools. It significantly speeds up the installation of dependencies and management of the virtual environment, making the development process smoother and faster. UV ensures consistent dependency resolution and environment setup across different development environments.</p> <ul> <li>UV, an extremely fast Python package and project manager, written in Rust</li> <li>Getting Started with UV<ul> <li>Install uv</li> <li>Install Python</li> <li>Install Python packages and dependencies</li> <li>Run Python code</li> <li>Launching JupyterLab</li> <li>Optional: Manage virtual environments manually</li> </ul> </li> </ul> <p>UV, an extremely fast Python package, virtual environment and project manager.</p> <ul> <li>\ud83d\ude80 A single tool to replace pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv, and more.</li> <li>\u26a1\ufe0f 10-100x faster than pip.</li> <li>\ud83d\udc0d Installs and manages Python versions.</li> <li>\ud83d\udee0\ufe0f Runs and installs Python applications.</li> <li>\u2747\ufe0f Runs single-file scripts, with support for inline dependency metadata.</li> <li>\ud83d\uddc2\ufe0f Provides comprehensive project management, with a universal lockfile.</li> <li>\ud83d\udd29 Includes a pip-compatible interface for a performance boost with a familiar CLI.</li> <li>\ud83c\udfe2 Supports Cargo-style workspaces for scalable projects.</li> </ul> <p>Specifically, UV is utilized for:</p> <ul> <li>Dependency Management: UV manages both project dependencies and development dependencies, ensuring that all necessary packages are installed quickly and efficiently.</li> <li>Virtual Environment Creation: UV creates and manages the project's virtual environment, isolating project dependencies from the global Python environment.</li> <li>Speed and Efficiency: UV's speed advantage over traditional tools like <code>pip</code> significantly reduces the time spent on environment setup and dependency installation, especially in projects with many dependencies.</li> <li>Consistency: By using UV, the project ensures that the development environment is consistent across different machines and platforms, reducing the risk of \"it works on my machine\" issues.</li> <li>Integration with pyproject.toml: UV leverages the <code>pyproject.toml</code> file for project configuration, making it easy to define and manage dependencies, scripts, and other project settings.</li> <li>Running Tools: UV is used to run development tools like <code>ruff</code>, <code>pytest</code>, and <code>toml-sort</code> directly from the command line, ensuring that the correct versions of these tools are used and that they are isolated from other projects. Example: <code>uv run ruff .</code></li> <li>Packaging: UV can be used to package the project for distribution, creating a <code>wheel</code> file that can be uploaded to PyPI or installed directly.</li> <li>Lockfile Management: UV uses a <code>uv.lock</code> file to ensure that all dependencies are installed at the exact versions specified, preventing compatibility issues and ensuring reproducibility.</li> </ul> <p>Key features of UV include:</p> <ul> <li>Written in Rust: UV is written in Rust, which provides excellent performance and memory safety.</li> <li>Compatibility: UV is compatible with <code>pip</code> and <code>virtualenv</code>, so it can be used with existing Python projects without requiring major changes.</li> <li>pyproject.toml Support: UV fully supports the <code>pyproject.toml</code> file, which is the recommended way to configure Python projects.</li> <li>Speed: UV is significantly faster than <code>pip</code> for most operations, especially when installing dependencies from scratch.</li> <li>Global Cache: UV uses a global cache to store downloaded packages, so they don't need to be downloaded again for each project.</li> </ul>"},{"location":"guides/uv/#getting-started-with-uv","title":"Getting Started with UV","text":"<p>Refer to the official UV - Getting Started - Documentation for detailed instructions on installing and using UV.</p> <p>This section guides you through the Python setup and package installation procedure using <code>uv</code> native commands over the <code>uv pip</code> interface and is based on the content of Sebastian Raschka's GitHub repository.</p> <p>[!NOTE] There are alternative ways to install Python and use <code>uv</code>. For example, you can install Python directly via <code>uv</code> and use <code>uv add</code> instead of <code>uv pip install</code> for even faster package management.</p> <p>If you prefer the <code>uv pip</code> commands, I recommend checking the official <code>uv</code> documentation.</p> <p>While <code>uv add</code> offers additional speed advantages, <code>uv pip</code> might be slightly more user-friendly for with your existing habits. However, if you're new to Python package management, the native <code>uv</code> interface is also a great opportunity to learn it from the start. It's also how I use <code>uv</code>, but I realized the barrier to entry is a bit higher if you are coming from <code>pip</code> and <code>conda/mamba</code>.</p>"},{"location":"guides/uv/#install-uv","title":"Install uv","text":"<p>Uv can be installed as follows, depending on your operating system.</p> <p>macOS and Linux</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>or</p> <pre><code>wget -qO- https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Windows</p> <pre><code>powershell -c \"irm https://astral.sh/uv/install.ps1 | more\"\n</code></pre>"},{"location":"guides/uv/#install-python","title":"Install Python","text":"<p>You can install Python using uv:</p> <pre><code>uv python install 3.10\n</code></pre>"},{"location":"guides/uv/#install-python-packages-and-dependencies","title":"Install Python packages and dependencies","text":"<p>To install all required packages from a <code>pyproject.toml</code> file (such as the one located at the top level of this GitHub repository), run the following command, assuming the file is in the same directory as your terminal session:</p> <pre><code>uv add . --group dev\n</code></pre> <p>[!NOTE] If you have problems with the following commands above due to certain dependencies (for example, if you are using Windows), you can always fall back to regular pip: <code>uv add pip</code> <code>uv run python -m pip install -U -r requirements.txt</code></p> <p>Note that the <code>uv add</code> command above will create a separate virtual environment via the <code>.venv</code> subfolder. (In case you want to delete your virtual environment to start from scratch, you can simply delete the <code>.venv</code> folder.)</p> <p>You can install new packages, that are not specified in the <code>pyproject.toml</code> via <code>uv add</code>, for example:</p> <pre><code>uv add packaging\n</code></pre> <p>And you can remove packages via <code>uv remove</code>, for example,   </p> <pre><code>uv remove packaging\n</code></pre>"},{"location":"guides/uv/#run-python-code","title":"Run Python code","text":"<p>Your environment should now be ready to run the code in the repository. You can test it by running the following command to run a python script:</p> <pre><code>uv run python main.py\n</code></pre> <p>Or, if you don't want to type <code>uv run python</code> every time you execute code, manually activate the virtual environment first.</p> <p>On macOS/Linux:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>On Windows (PowerShell):</p> <pre><code>.venv\\Scripts\\activate\n</code></pre> <p>Then, run:</p> <pre><code>python main.py\n</code></pre>"},{"location":"guides/uv/#launching-jupyterlab","title":"Launching JupyterLab","text":"<p>You can launch a JupyterLab instance via:</p> <pre><code>uv run jupyter lab\n</code></pre>"},{"location":"guides/uv/#optional-manage-virtual-environments-manually","title":"Optional: Manage virtual environments manually","text":"<p>Alternatively, you can still install the dependencies directly from the repository using <code>uv pip install</code>. But note that this doesn't record dependencies in a <code>uv.lock</code> file as <code>uv add</code> does. Also, it requires creating and activating the virtual environment manually:</p> <p>1. Create a new virtual environment</p> <p>Run the following command to manually create a new virtual environment, which will be saved via a new <code>.venv</code> subfolder:</p> <pre><code>uv venv --python=python3.12\n</code></pre> <p>2. Activate virtual environment</p> <p>Next, we need to activate this new virtual environment.</p> <p>On macOS/Linux:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>On Windows (PowerShell):</p> <pre><code>.venv\\Scripts\\activate\n</code></pre> <p>3. Install dependencies</p> <p>To install the required dependencies from your local <code>requirements.txt</code> file, use the following command:</p> <pre><code>uv pip install -r requirements.txt\n</code></pre>"},{"location":"notebooks/app_without_fastapi/","title":"Jupyter Notebook - App","text":"In\u00a0[\u00a0]: Copied! <pre>import warnings\nfrom typing import List, Tuple\n\nfrom pydantic import BaseModel, Field\nfrom torch import rand\nfrom torch.nn import Sequential\nfrom typing_extensions import Annotated\n\nfrom lit_auto_encoder.auto_encoder import LitAutoEncoder\nfrom lit_auto_encoder.train_autoencoder import train_litautoencoder\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import warnings from typing import List, Tuple  from pydantic import BaseModel, Field from torch import rand from torch.nn import Sequential from typing_extensions import Annotated  from lit_auto_encoder.auto_encoder import LitAutoEncoder from lit_auto_encoder.train_autoencoder import train_litautoencoder  warnings.filterwarnings(\"ignore\") In\u00a0[\u00a0]: Copied! <pre># Input validation model\nclass NumberFakeImages(BaseModel):\n    n_fake_images: Annotated[int, Field(ge=1, le=10)]  # type: ignore # Between 1 and 10 fake images allowed\n</pre> # Input validation model class NumberFakeImages(BaseModel):     n_fake_images: Annotated[int, Field(ge=1, le=10)]  # type: ignore # Between 1 and 10 fake images allowed In\u00a0[\u00a0]: Copied! <pre>def train_model() -&gt; Tuple[Sequential, Sequential]:\n    \"\"\"Train the autoencoder model.\n\n    Returns:\n        tuple[Sequential, Sequential]: Encoder and decoder models.\n    \"\"\"\n\n    encoder, decoder, _is_model_trained = train_litautoencoder()\n    return encoder, decoder\n</pre> def train_model() -&gt; Tuple[Sequential, Sequential]:     \"\"\"Train the autoencoder model.      Returns:         tuple[Sequential, Sequential]: Encoder and decoder models.     \"\"\"      encoder, decoder, _is_model_trained = train_litautoencoder()     return encoder, decoder In\u00a0[\u00a0]: Copied! <pre># Train encoder and decoder\nencoder, decoder = train_model()\n</pre> # Train encoder and decoder encoder, decoder = train_model() <pre>GPU available: True (mps), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n\n  | Name    | Type       | Params | Mode \n-----------------------------------------------\n0 | encoder | Sequential | 50.4 K | train\n1 | decoder | Sequential | 51.2 K | train\n-----------------------------------------------\n101 K     Trainable params\n0         Non-trainable params\n101 K     Total params\n0.407     Total estimated model params size (MB)\n8         Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Epoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 203.43it/s, v_num=80]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <pre>Epoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 198.91it/s, v_num=80]\n</pre> In\u00a0[\u00a0]: Copied! <pre>def create_embed(\n    input_data: NumberFakeImages, encoder: Sequential, decoder: Sequential, checkpoint_path: str\n) -&gt; List[List[float]]:\n    \"\"\"Embed fake images using the trained autoencoder.\n\n    Args:\n        input_data (NumberFakeImages): Input data containing the number of fake images to embed.\n        encoder (Sequential): Encoder model.\n        decoder (Sequential): Decoder model.\n        checkpoint_path (str): Path to the checkpoint file.\n\n    Returns:\n        List[List[float]]: A list containing the embeddings of each fake images as a list.\n    \"\"\"\n\n    n_fake_images = input_data.n_fake_images\n\n    # Load the trained autoencoder from the checkpoint\n    autoencoder = LitAutoEncoder.load_from_checkpoint(\n        checkpoint_path, encoder=encoder, decoder=decoder\n    )\n    encoder_model = autoencoder.encoder\n    encoder_model.eval()\n\n    # Generate fake image embeddings based on user input\n    fake_image_batch = rand(n_fake_images, 28 * 28, device=autoencoder.device)\n    embeddings = encoder_model(fake_image_batch)\n\n    return embeddings.tolist()\n</pre> def create_embed(     input_data: NumberFakeImages, encoder: Sequential, decoder: Sequential, checkpoint_path: str ) -&gt; List[List[float]]:     \"\"\"Embed fake images using the trained autoencoder.      Args:         input_data (NumberFakeImages): Input data containing the number of fake images to embed.         encoder (Sequential): Encoder model.         decoder (Sequential): Decoder model.         checkpoint_path (str): Path to the checkpoint file.      Returns:         List[List[float]]: A list containing the embeddings of each fake images as a list.     \"\"\"      n_fake_images = input_data.n_fake_images      # Load the trained autoencoder from the checkpoint     autoencoder = LitAutoEncoder.load_from_checkpoint(         checkpoint_path, encoder=encoder, decoder=decoder     )     encoder_model = autoencoder.encoder     encoder_model.eval()      # Generate fake image embeddings based on user input     fake_image_batch = rand(n_fake_images, 28 * 28, device=autoencoder.device)     embeddings = encoder_model(fake_image_batch)      return embeddings.tolist() In\u00a0[\u00a0]: Copied! <pre># Create embeddings\nembeddings = create_embed(\n    NumberFakeImages(n_fake_images=2),\n    encoder,\n    decoder,\n    checkpoint_path=\"./lightning_logs/LitAutoEncoder/version_0/checkpoints/epoch=0-step=100.ckpt\",\n)\n\n# Print the embeddings\nprint(\"\u26a1\" * 20, \"\\nPredictions (image embeddings):\\n\", embeddings, \"\\n\", \"\u26a1\" * 20, sep=\"\")\n</pre> # Create embeddings embeddings = create_embed(     NumberFakeImages(n_fake_images=2),     encoder,     decoder,     checkpoint_path=\"./lightning_logs/LitAutoEncoder/version_0/checkpoints/epoch=0-step=100.ckpt\", )  # Print the embeddings print(\"\u26a1\" * 20, \"\\nPredictions (image embeddings):\\n\", embeddings, \"\\n\", \"\u26a1\" * 20, sep=\"\") <pre>\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\nPredictions (image embeddings):\n[[0.3581562042236328, 0.14595450460910797, 0.47827404737472534], [0.5079143643379211, 0.09660176187753677, 0.5766311287879944]]\n\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/app_without_fastapi/#jupyter-notebook-app","title":"Jupyter Notebook - App\u00b6","text":""},{"location":"notebooks/app_without_fastapi/#run-application-interactively-without-using-fastapi","title":"Run Application interactively without using FastAPI\u00b6","text":""},{"location":"notebooks/app_without_fastapi/#train-the-model","title":"Train the Model\u00b6","text":""},{"location":"notebooks/app_without_fastapi/#create-embeddings","title":"Create Embeddings\u00b6","text":""}]}